---
title: "Age Models in Large Scale Synthesis"
author:
- affiliation: University of Arizona
  name: Andria Dawson
- affiliation: University of Wisconsin - Madison
  name: Simon Goring (co-authored)
- affiliation: University of Wisconsin - Madison
  name: John Williams
bibliography: styles/baconizing.bib
output:
  html_document:
    code_folding: show
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    self_contained: yes
    theme: readable
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    reference_docx: styles/word_template.docx
    toc: yes
    toc_depth: '3'
dev: svg
highlight: tango
keywords: chronology, geochronology, paleoecology, age-models, Bacon, 210Pb, 14C,
  radiocarbon
csl: styles/elsevier-harvard.csl
abstract: Well constructed chronologies are critical to reliable paleoecological inference.
  Hence, a first step in large-scale paleodata syntheses is to review and, as needed,
  revise chronologies.  Here we demonstrate and document the systematic construction
  of age models from the Neotoma Paleoecological Database, as a way of examining the
  assumptions and decisions involved in model construction. We use the Bacon age modeling
  software, implemented in R, and catalogue several of the challenges and decision-making
  process necessary for chronology development. This paper examines the biases resulting
  from posthoc calibration of default radiocarbon chronologies, ^210^Pb uncertainty,
  the treatment of inflection points in sedimentation rates, the use biostratigraphic
  markers associated with known historical events, handling of accumulation rates
  that may change over time, memory parameters, section and thicknesses within Bacon,.
  One third of age models in Neotoma have default chronologies expressed in radiocarbon
  years, and simply calibrating interpolated ages results in differences of between
  31 and 470 cal yrs BP when compared to rebuilding age models. ^210^Pb dates XXX
  The resulting age models show that section thickness generally increases with absolute
  model age. Accumulation rate clearly change over time, this shift happens rapidly
  within upper sediments and must be accounted for in the Bacon model settings. Reported
  ^210^Pb age uncertainties within the model framework are, most often, significantly
  lower than the estimated model uncertainties. Within legacy records, where ^210^Pb
  dates are unreported, modeled 210Pb uncertainties based on reported dates, provide
  a reasonable framework for re-building chronologies. ADD - SUMMARIZE SOLUTIONS –
  BOTH AS-IS AND FUTURE.
---

# Introduction

```{r load_libraries, echo=FALSE, message=FALSE, warnings = FALSE, results='hide'}

library(purrr, verbose = FALSE)
library(purrrlyr, verbose = FALSE)
library(ggplot2, verbose = FALSE)
library(neotoma, verbose = FALSE)
library(plyr, verbose = FALSE)
library(reshape2, verbose = FALSE)
library(mgcv, verbose = FALSE)
library(Bchron, verbose = FALSE)
library(viridis, verbose = FALSE)
library(dplyr, verbose = FALSE)
library(readr, verbose = FALSE)
library(gridExtra, verbose = FALSE)

version <- 0.1

```

Recent advances in computing have improved our ability to store and process large complex data sets. These advances have motivated scientists to address hypotheses that rely on easy-access to paleoecological data sets covering specified and often large domains in space and time. Paleoecological databases streamline this process by providing the means to store, curate, and query structured data. These databases facilitate analysis over large spatio-temporal scales, but also on smaller local scales, and provide the opportunity to more easily undertake synthetic analysis and extend current understanding [@brewer2012paleoecoinformatics;@uhen2013card]. Most records stored in paleoecological databases contain data from paleo-ecological or -climate proxy samples and the chronology or ages of these samples, which is usually the result of the application of an age-depth model. It is often the case that paleoecologists wish to make inference about processes through time, and in this case the chronology assigned to a sedimentary archive can strongly affect the interpretation of a record. Paleoecologists have been aware of the varying methodologies and limitations in age modeling capabilities for some time, and the issues that these limitations cause are well noted [@grimm2009magnitude;@liu2012temporal]. 

Towards methodological consistency, recent efforts have focused on standardizing age models for records within a database [@giesecke2014towards] and establishing regional benchmarks [@blois2011methodological;@flantua2016geochronological]. Age model standardization in Europe relied on the Clam software [@blaauw2010methods], as did efforts with the North American Pollen Database [@blois2011methodological]. Towards methodological soundness, the paleoecological community has begun to recognize value in the explicit accounting of uncertainty in processes. Recent developments in age modelling include several Bayesian approaches that allow for the quantification of age uncertainty in sediment records [bacon, oxcal, bchron, ...?]. With the the development of tools to quantify age uncertainty, and the desire to include and propogate uncertainty in paleoecological analyses comes the need for databases to store these uncertainty estimates. This desire presents new challenges to database managers and developers by increasing data structure complexity and storage requirements.

Despite recent methodological advances in age modeling, many paleoecological records preceding these developments include extant age models. As new and extant methods diverge, the limitations of age models developed using extant methods become more problematic. [@blaauw2012out] noted that the subjectivity and flexibility in age-depth modeling may introduce additional "researcher degrees of freedom" that can lead to greater rates of false-positive relationships in paleoecological research. However, in practice it is difficult to identify and quantify these effects on existing age-models, especially when only a limited description of methodology is available. The reliability of extant age models may be questionable for at least two reasons: 1) the lack robust of uncertainty quantification, 2) their construction was carried out in radiocarbon years, and 3) inconsistency in methodology resulting in potential biases. However, the value of these extant records cannot be ignored; it is the inherent value of these records that motivated these (and probably other) standardization efforts.

Reliable sediment age estimates require accurate dating of high quality stratigraphic control points, well supported conversion from radiometric (often radiocarbon) to calendar years, robust and reliable models that mehcanistically or empirically represent the underlying process of sediment deposition and accumulation, and clear quantification of uncertainty that is able to take into account measurement and process uncertainty.

Chronology construction is dependant on the availability of dated stratigraphic control points from a sediment core. These control points may be geochronological (dated material), geostratigraphic (e.g., the “modern” core top), and sometimes biostratigraphic (changes in pollen assemblages associated with dated changes on the landscape). Geochronological control point ages are often uncertain due to analytical errors during the laboratory radiocarbon dating process [@ward1978procedures], the conversion of radiocarbon to calendar years [@reimer2013selection], and potential differences between the ages of macrofossil material and age of sediment [@blois2011methodological]. Geostratigrahic markers may have fewer sources of uncertainty -- the core top age is assumed to be the year of sampling -- although sedimentary mixing of the upper sediment during sampling may introduce some uncertainty. Finally, biostratigraphic control points are determined by the examination (usually visual) of changes in pollen assemblages throughout a core. However, time series of pollen counts are noisy, and in practice identifying changes in composition is both difficult and subjective [@dawson2016quantifying]. There remains uncertainty in the identification of compositional shifts related to the chronological/stratigraphic sampling density [@liu2012temporal], and, in the case of landscape-scale phenomena such as the mid-Holocene Hemlock decline [@bennett2002determining;@davis1981outbreaks], the need to assign temporal bounds to the landscape-scale phenomenon that caused the compositional shift.

Chronologies are, ultimately, approximations of the physical processes leading to variable sediment accumulation within depositional basins.  While methods such as Bacon [@blaauw2011flexible] may include autocorrelation terms intended to reflect physical processes, they do not explicitly attempt to model the physical process of sedimentation.  Recent theoretical work extends our understanding of sedimentation within lake basins [@bennett2016interpretation], describing broad relationships between basin shape and long term patterns of accumulation that can improve the mechanistic characterization of sedimentation processes. Varved lake records can act as controls to validate model precision and accuracy [@trachsel2017all].  Recent work compiling published varved records [@ojala2012characteristics] provides further insight into chronology accuracy. However varved records are often spatially clustered, and may be generated through a range of physical, biological and climatic conditions, indicating the possibility that these systems may not reflect broader controls on non-laminated (or varved) lacustrine systems.

The development of the first radiocarbon calibration curve [IntCal98; @hughen1998intcal98] allowed for the association of radiocarbon years with calendar years, based on material with known ages and associated ^14^C dates. Continued improvement of the calibration curve has resulted in improved chronology development, but chronologies are rarely updated as new curves are generated [@grimm2014working]. Offset and uncertainty are influenced by the method used to re-calibrating legacy models using only radiocarbon years. Comparisons between re-calibrated age models against directly re-calibrated dates highlight two issues: the possibility of systematic offsets resulting in biases in age estimates, and the need for direct intervention and systematic rules-based chronology construction in synthesis work.

```{r get_cores, echo=FALSE, message=FALSE, results='hide', warning=FALSE}

source('R/getNAcores.R')

all_downloads <- north_american_cores(version)

if (paste0('pubs_v', version, '.rds') %in% list.files('data/output')) {
  pubs <- readRDS(paste0('data/output/pubs_v', version, '.rds'))
} else {
  pubs <- neotoma::get_publication(all_downloads)
  saveRDS(pubs, file = paste0('data/output/pubs_v', version, '.rds'))
}

meta.table <- pubs %>% 
  map(function(x){
    metas <- do.call(rbind.data.frame, 
                     lapply(x, '[[', 'meta'))
    output <- metas[which.min(metas$year),]
    if(nrow(output) == 0) {
      output <- data.frame(id = NA, 
                           pub.type = NA, 
                           year = NA,
                           citation = NA)}
    return(output)}) %>% 
  bind_rows

chron <- sapply(all_downloads, function(x){
  types <- sapply(x$chronologies, function(x) x$age.type[1])
  age_types <- types[which.min(match(types, c("Calendar years BP",
                                              "Varve years BP",
                                              "Calibrated radiocarbon years BP",
                                              "Radiocarbon years BP")))]
  if(length(age_types) == 0) { age_types <- NA }
  return(age_types)})

meta.table$chron <- NA
meta.table$chron <- chron

source('R/section_1_scripts.R')

```

<object type="image/svg+xml" data="figures/agediff_plot_final.svg" style="width: 100%;">
Differences between age models and direct recalibration.
  <!-- fallback image in CSS -->
</object>

**Figure 1**. *Diagram of two methods for generating calibrated dates from radiocarbon age-models.  (a) Each age (in radiocarbon years) is calibrated directly (red arrows) from an existing model (black dashed line) or (b) the age models is reconstructed using linear interpolatation (red dashed line) from calibrated radiocarbon ages (red line).  Negative values indicate that recalibration of interpolated ^14^C dates followed by linear interpolation of calibrated ages provides systematically older ages than direct recalibration of interpolated ages. (d) Using the chronological controls of the default age model from Neotoma records, calibrate chronological controls reported in radiocarbon years, and then use linear interpolation.  Any records with age-reversals were rejected to simplify this illustrative example.*

```{r, plot.chron.change, fig.width=6, echo=FALSE, message=FALSE, results='hide', warning=FALSE, dev='svg'}

source('R/figures/ageplot_histogram.R')

chronology_type_histogram(meta.table)

```

<object type="image/svg+xml" data="figures/histograms_cleaned.svg">
  Histograms of age model types against year of publication 
  <!-- fallback image in CSS -->
</object>

**Figure 2**.  *Number and type of default chronologies for North American pollen records in the Neotoma Paleoecological Database based on the original year of publication for the dataset.  Within Neotoma the default dataset chronology may be updated by subsequent researchers when this information is provided to a data steward.*


In the paper we take the position that develping new age models based on recalibrated stratigraphic control points that account for uncertainty is the ideal approach. However, large scale data analysis across heterogeneous data can be complex, thus the use of harmonized age models, that integrate ideas about the underlying mechanics of the processes linking depth and time become important [@blaauw2011flexible]. The Neotoma database [@grimm2008neotoma] contains `r length(get_dataset(datasettype="pollen", ageold = 2000, ageyoung = -70))` global pollen records that can be used for paleoecological analysis.  These records have been obtained from publications that span a time period from  `r min(meta.table$year, na.rm=TRUE)` to `r max(meta.table$year, na.rm=TRUE)`, with more than half the records coming from before 1983.

This paper builds on work from Dawson *et al*. [@dawson2016quantifying], which rebuilt existing age models using the Bacon age-depth model, by explicitly detailing the key challenges posed by generating a large number of Bayesian age models, the solutions available, and quantifying the effects of decisions on aggregate outcomes, in the process of rebuilding models. In this paper we outline the decision making process around the parameter selection for the age models, summarize differences between the original chronologies and the new chronologies, highlight differences between chronologies generated using two age model types and highlight best practices for chronologies within large-scale synthesis.  We also identify limitations of current methods, that could serve to improve chronology construction in the future.

# Key Issues in Generating Age Models from Neotoma

## Use of posthoc calibration of individual ages in radiocarbon chronologies

Across the Neotoma database (here we refer to North America only), many records within the database still record chronologies using only radiocarbon years (Figure 2). The transition from age models using only radiocarbon years to those with calibrated radiocarbon years within Neotoma is dramatic.  The final radiocarbon model appears to be from 1998, following this we see no more radiocarbon models.  Along with this transition, we are seeing a second transition from simple linear models to more complex models using flexible Bayesian methods.  A critical question then becomes, when faced with records generated using only uncalibrated dates, should we calibrate radiocarbon dates, generate age models *de novo*, or ignore the records altogether?

While not the preferred method, direct recalibration of interpolated ages does occur within the Neotoma ecosystem. For example, the temporal search function within the Neotoma Explorer (http://apps.neotomadb.org/explorer), Tilia (http://tiliait.org) and the Neotoma API (http://api.neotomadb.org) all use a lookup table that directly recalibrates ages in radiocarbon years. However, this process results in systematic biases in both synthetic data and in the actual Neotoma data (Figure 1).

In the paper we take the position that develping new age models based on recalibrated stratigraphic control points that account for uncertainty is the ideal approach. However, large scale data analysis across heterogeneous data can be complex, thus the use of harmonized age models, that integrate ideas about the underlying mechanics of the processes linking depth and time become important [@blaauw2011flexible]. Developments since Blois *et al*. [-@blois2011methodological], and workshops such as the PAGES-sponsored Age Models, Chronologies, and Databases Workshop [@grimm2014working] have begun the process of outlining the methods required to undertake large-scale efforts to re-build age models.

## Choice of Age Model Type and Software

Estimating age-depth relationships and uncertainties requires that decisions be made about the type of age modeling approach, choice of software implementation, suitable chronological controls and model parameters [@blois2011methodological;@grimm2014working;@giesecke2014towards]. We here pursue Bayesian approaches for their ability to flexibily estimate the ages of pollen samples with robust uncertainty. Several Bayesian age modelling software packages exist, including Bacon, Bcal, Bchron, and Oxcal. We use Bacon because it 1) implements a Bayesian framework that estimates sample age posteriors, 2) is widely used by paloecologists, and 3) accounts for some of the key components in the sediment deposition and accumulation process. Using Bacon to estimate age-depth relationships requires the specification of: 1) chronological controls, which include radiometric dates, biostratigraphic markers, and their uncertainty, 2) priors on the accumulation rate and memory, 3) values for the resolution and structure of divisions within the sediment core (section thicknesses and hiatuses). To ensure a consistent methodology among sediment core in the UMW domain, we developed a set of standard decisions that are described below.

## Choice of Age Controls and Types

Neotoma contains `r nrow(neotoma::get_table('ChronControlTypes'))` different chronology control types, including various pollen stratigraphic controls, radiocarbon and other isotopic dates and marine isotope stages.  Since this analysis was focused primarily on sedimentary pollen records, most of the chronological controls used reflect more recent control types, including ^210^Pb, ^137^Cs, and ^14^C dates, as well as biostratigraphic dates associated with events of known or inferred dates, such as historical fires and the *Tsuga* decline.  Core tops are often used, and have highly certain dates, however, several older records use interpolated dates for the core top when sediment was lost in the coring process.  These cores should be treated with caution, and this is often noted in the core description.

```{r, results = 'hide', echo = FALSE, warning=FALSE}
if('controls.RDS' %in% list.files('data/output/rds_files/')) {
  controls <- readRDS('data/output/rds_files/controls.RDS')
} else {
  controls <- lapply(all_downloads, function(x)try(get_chroncontrol(x)))
  for(i in length(controls):1){if('try-error' %in% class(controls[[i]])) {controls[[i]] <- NULL }}
  saveRDS(controls, 'data/output/rds_files/controls.RDS')
}

control_type <- controls %>% 
  map(function(x)data.frame(control = as.character(x$chron.control$control.type))) %>% 
  bind_rows() %>% 
  group_by(control) %>% 
  summarise(count = n())

knitr::kable(control_type)
```

## Chronological Uncertainty

Methods of understanding an managing radiocarbon uncertainty are well established, however the use of ^210^Pb and biostratigraphic dates are less well understood in the context of sedimentary archives, particularly in large scale synthesis work. Biostratigraphic events often require indentification by the researcher and may be time-transitive while uncertainties associated with ^210^Pb may appear to narrow to properly model ages using the Bacon software.

### Dealing with zero-value ^210^Pb errors

Bacon requires that all chronological markers be associated with defined errors. Historically, some ^210^Pb data entered into Neotoma were entered without error reporting.  Of the 398 ^210^Pb age controls in Neotoma, 148 have no error reported (Figure 3).  Binford [-@binford1990calculation] reports *"Ninety-five per cent confidence intervals range from about 1 -- 2 years at 10 years of age, 10 -- 20 at 100 years, and 80 -- 90 at 150 years old."*  Using this assessment we fit a smooth linear function to assign 95% confidence intervals for all ^210^Pb dates with missing uncertainty data.  These confidence intervals were then divided by 2 (and rounded up to the nearest integer) to obtain standard deviations to be used in the Bacon model (Figure 3). 

```{r, echo=FALSE, message=FALSE, results='hide', warning=FALSE, dev='svg'}

all_geochron <- readRDS(paste0('data/output/all_geochron_v', version, '.rds'))

source('R/lead_plotting.R')

lead_plots(all_geochron)

```

**Figure 3.** *Reported uncertainty for ^210^Pb dates within Neotoma, by age, with the total count of unassigned samples by age bin (lower).  Estimated uncertainty, using a model relating reported ^210^Pb uncertainty to age, is then assigned to all ^210^Pb dates with unreported uncertainty. Present is defined as radiocarbon present, or 1950 CE.* 

### Expert elicitation exercise to identify biostratigraphic events

Biostratigraphic events have been used extensively as chronological controls within age models, but are often poorly constrained with respect to the identification of the "rise" event. The *Ambrosia* rise is a well known phenomenon in the eastern United States, and elsewhere, that appears to be contemporaneous with Euro-American settlement, land clearance and the initiation of intensive agriculture in the region [@McAndrews1968;@mcandrews1988human]. In the Upper Midwest, significant increases in *Ambrosia*, *Rumex*, and/or Poaceae are typically coincident with the settlement horizon. Because of this feature it is possible to establish a biostratigraphic chronological control. When the rise can be identified visually it can be used as a stratigraphic marker with an age defined as either immediately preceeding (when the "pre-settlement" sample is used), or immedately after.  The pre-settlement sample has been identified for many of the fossil pollen cores in the Neotoma database.  To reduce expert bias in the selection of the 'pre-settlement' sample we asked a team of experts to identify pre-settlement samples based on pollen diagrams depicting proportional changes as a function of depth for key indicator species and the ten most abundant arboreal taxa, with no temporal scale.  More details of this procedure are available in Dawson et al. [-@dawson2016quantifying] and in Kujawa et al. [@kujawa2016theeffect].

The elicitation exercise provides an independently assesed estimate of biostratigraphic change in the pollen records that can be used as a biostratigraphic marker. Gridded Public Land Survey (PLS) datasets in this region provide a historical record of the minimum date of presence for legacy forests [@goring2016novel]. Goring et al. [-@goring2016novel] use historical records of forest cover from the 1820s - 1903 to provide estimates of forest cover prior to EuroAmerican settlement.  The gridded datasets associated with the paper also provides the PLS sampling year; we subtract 50 from the value for grid cells in which sites fall to assign a "pre-settlement" period for each core, with an age uncertainty (stanadard deviation) of 50 years.  Ideally, methods would include asymmetric uncertainty, since, in the case of the settlement horizon, we know the minimum age of, but not the most recent age for potential settlement.

Five cores were located within grid cells without digitized PLS data. In these cases, we look to the surrounding grid cells to estimate the maximum sampling year. One site in the Upper Peninsula of Michigan was assigned a maximum sampling year of 1860. The four remaining cores were in the Lower Peninsula of Michigan, and were assigned a maximum year of sampling of 1840.

## Bacon Settings

### Accumulation Rate Priors

Bacon provides a default accumulation rate for all depths based on Goring *et al*. [-@goring2012deposition]'s survey of Holocene accumulation rates in eastern North America.  Empirical age-depth curves suggest that the age-depth relationship is non-linear, as a result of lower sediment compaction in upper sediments [@goring2012deposition], basin shape [@bennett2016interpretation] and patterns of deposition and sediment transport operating on longer time scales [@goring2012deposition;@webb1988rates] and acceleration of erosion rates during the Anthropocene [REF]. Narrowing the geographic and temporal range for the target reconstructions made it neccessary to re-asses mean accumulation rates, focusing on a narrower temporal and spatial window.  Regionally, there is evidence that after the time of settlement, sediment accumulation is much faster (fewer years per cm of sediment accumulation) than before settlement (more years per cm of sediment accumulation), although bulk density may decrease.

Many sites show an inflection point in sedimentation rates at or around the Anthropocene horizon in the Upper Midwest.  This is likely a combination of both decreased compation in upper sediments and changes in land use and erosion. To generate accumulation rate priors for the Upper Midwest we estimated accumulation rates by pooling the mean age and change in depth for adjacent chronological markers. These accumulation rates were pooled into "modern" and "pre-settlement" groups. For each group a mean equal to the empirical mean of accumulation rates for that time period, and a variance equal to double the empirical variance from the grouped accumulation rates was calculated.  Doubling the empirical variance represents a conservative approach to modeling uncertainty, in that it accounts for additional variability in accumulation rates that was not observed in the data.

The modern accumulation rate prior, from more recent sedimentation rates was assigned to the post-settlement portion of cores with "settlement" horizons assigned through the Expert Elicitation. The 2K prior was assigned for the pre-settlement portion of the cores, and for the entirety of cores without identified "pre-settlement" horizons.  Cores with two priors were assigned an "instantaneous" (10yr) hiatus at the pre-settlement sample depth.

### Determining section thickness

Bacon works by diving a core into sections whose lengths are determined by the user [@blaauw2011flexible]. Sections are the atomic unit of a Bacon model, and various parameters are fit within each section.  In particular, the `memory` parameter defines the flexibility of the accumulation rates between adjacent sections, thus a model with large section thicknesses will be, by virtue of having fewer overall sections, less flexible. However, narrow section thicknesses result in very large run-times, and may show other, unanticipated problems associated with finding fits in multi-parameter space.

Dividing a core into sections is an approximation of time discretization [REF - Andria?], abstracted through the process of deposition.  Consistent section widths across all cores (and thus consistent discretization) should be a goal. However, the internal unmeasurable variability in sedimentation rates, and uncertainty in the age-depth models themselves, makes it difficult to implementat consistent section thicknesses.  To provide prescriptive widths, but still allow flexibility as needed in the Bacon modelling, each core was run with widths of 5, 10, 15 and 20cm.  Model fit was assessed visually, and the best fit model was subsequently chosen to be the default calibrated age model for that core.

# Results

## Age Controls

### ^210^Pb Errors

Bacon uncertainty estimates within the chronologies are consistently larger than the ^210^Pb error estimates for the point samples, while for most records the age estimates fit neatly along the 1:1 line  (Figure 4). 

```{r, lead_comparison, results = 'hide', echo = FALSE, warning=FALSE, dev='svg'}

source('R/lead_ages.R')
comp_doc <- compare_lead()

comp_doc[[1]]
```

**Figure 4**. *Reported and estimated ^210^Pb ages from the Bacon models show strong accordance, for the most part.  Uncertainties within the constructed Bacon models at the depths of the ^210^Pb samples show higher uncertainty from Bacon models than from the original uncertainty models. The autoregressive nature of the Bacon model, the influence of the memory parameter, and the nature of the Bayesian model itself mean that uncertainty is propagated through the core, and as such low uncertainty in the individual date (x-axis) does not result in low Bacon uncertainty.*

Only three lakes in the region had original age models that included estimates of uncertainty for a chronology that included ^210^Pb dates.  For these records we see that Bacon estimates higher model uncertainty than the original age model, but that the difference in modeled uncertainty is highly site-specific (Figure 5).  For Crooked Lake [@brugam1997holocene] the uncertainty difference is high and consistent with depth/age (triangles, Figure 5), while for Fish Lake [@umbanhowar2004interaction] the difference is small (squares; Figure 5).  Brown's Bay shows the highest uncertainty, but only at the deepest depth (circles; Figure 5).

```{r, error_by_age, results = 'hide', echo = FALSE, warning=FALSE}

lead_out <- comp_doc[[2]]

drop_outlier <- lead_out %>% 
  filter(bacon_error1 < 200) %>% 
  select(site.name, age, bacon_error1, e.older) %>% 
  na.omit() %>% 
  arrange(site.name, age)

lead_comp_plot <- ggplot(data = drop_outlier) + 
  geom_point(aes(x = age, y = bacon_error1, shape = site.name), 
             alpha = 0.7, size = 3) +
  geom_point(aes(x = age, y = e.older, shape = site.name), 
             color = 'red', alpha = 0.7, size = 3) +
  geom_ribbon(aes(x = age, ymin = e.older, ymax= bacon_error1, group = site.name),
              alpha = 0.1, color = alpha(colour = 'black', 0.2)) +
  geom_segment(aes(x = age, xend = age, 
                   y = bacon_error1, yend = e.older), alpha = 0.3) +
  scale_x_continuous(limits=c(-50, 100)) +
  scale_y_continuous(limits=c(0, 80)) +
  coord_equal(expand = c(0,0)) +
  xlab('Years Before Present') +
  ylab('Model Uncertainty') +
  annotate("text", x = -25, y = 80, label = "Bacon Uncertainty") +
  annotate("text", x = -25, y = 75, label = "210Pb Uncertainty", color = "red") +
  theme_bw() +
    theme(axis.title.x = element_text(family = 'serif', 
                                      face = 'bold.italic', 
                                      size = 18),
          axis.title.y = element_text(family = 'serif', 
                                      face = 'bold.italic', 
                                      size = 18),
          axis.ticks = element_blank(),
          axis.text.x = element_text(family = 'serif', 
                                     face = 'italic', 
                                     size = 14),
          axis.text.y = element_text(family = 'serif', 
                                     face = 'italic', 
                                     size = 14))

ggsave(filename = 'figures/lead_comp_plot.svg', plot = lead_comp_plot, width = 8, height = 5)

```

<object type="image/svg+xml" data="figures/lead_comp_final.svg">
  Comparison of 210Pb dates measured and reconstructed error at those depths. 
  <!-- fallback image in CSS -->
</object>

**Figure 5**. *For the three cores within the region with both reported ^210^Pb and reconstructed Bacon uncertainties, the latter are consistently higher, although the magnitude of the difference varies by record.  Fish Lake shows the smallest difference, while reconstructed ages for Brown's Bay shows the largest difference.*

### Expert Elicitation

```{r, elecitation_plot, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
source('R/elicitation_plot.R')

elicitation_plots <- expert_elicitation()

```

On average, the original modeled ages were older than the biostratigraphic ages assigned based on the dates for PLS in the vicinity of the focal lakes.  Bacon models showed better affinity to the assigned biostratigraphic dates based on expert identification of the *Ambrosia* rise in the region.  This age differential means that Bacon models are, on average, 44 years younger at the settlement horizon.

Of the 163 records with assigned settlement ages from the elicitation exercise, 55 of the original age models have a modeled ages at the assigned settlement horizon that are younger than 1800 CE, while only 17 of the Bacon models have a modeled age for the settlement horizon younger than 1800 CE.


```{r, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, dev = 'svg'}
elicitation_plots[[1]]
```

**Figure 6**. *The relationship between assigned 'settlement' horizons, based on Public Land Survey records and the expert elicitation exercise, and the modeled age of the horizon within the final age-depth model for each individual record.  The settlement horizon plotted against the modeled age within the original age model (left); and against the newer Bacon model (right).  A best fit linear regression is plotted in blue, with a 1SE envelope in gray. The negative relationship in the original models may reflect an over estimation of sedimentation rates in upper sediments as a result of linear age-model fits. Higher variability in the orginal relationship may represent greater inter-researcher variability in settlement horizon estimation.*

The linear models fit to the settlement horizons in show a negative slope for the relationship between the assigned settlement age and the modeled age (Figure 6). This may be because many of the original linear models over-estimate sedimentation rates in the upper sediments since they assign an inflection point at the horizon.  Higher variability is likely a result of both variability in detecting and assigning the settlement horizon among researchers, but also in the process for assigning the age associated with settlement regionally.  Researchers may have used historical documents, oral histories or other records.  The use of the standardized PLS dates in te new models reduced this source of variability.

## Bacon Settings

### Accumulation Rate Priors

**Table 1**. *Empirical estimates of accumulation rates in the Upper Midwestern United States from Neotoma records.*

 Group   Mean	   Variance	    Rate	   Shape
------ -------- ----------- -------- -------
Modern	3.02	     5.70	      0.53     1.6
2K	    15.00	     250	      0.06	   0.9


Accumulation rates (*yr/cm* of sediment accumulation) for records in the region show clear differentiation between accumulation rates within the last 200 years and accumulations prior to the last 200 years.  Means and variance are presented in **Table 1** along with the Gamma rate and shape parameters.  Accumulation rates (the mean *yr cm^-1^* deposition) change dramatically, dropping by five-fold in the modern period.

```{r, get_acc_rates, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, dev = 'svg'}

source('R/acc_rate_priors.r')

plot_acc_rates(accs)

```

**Figure 7**. *Sediment accumulation rates for cores in the study region with samples in the last 2000 years (in calibrated radiocarbon years, with 0 at 1950 CE).  The upper panel shows the accumulation rate in years per cm against the interval midpoint age in calibrated radiocarbon years. The dashed vertical line indicates 1850 CE, the approximate date used to represent major land-use change as a result of EuroAmerican settlement. Note the strong change in accumulation rates between the periods before and after EuroAmerican settlement. The lower panel shows the distribution of accumulation rates for the pre- and post settlement eras, highlighting the difference in central tendency.*

Deposition rates increase with time across the pre-/post-settlement interval (Figure 7a), however Bacon does not support the ability to allow time-dependent priors, except through the use of sequential hiatuses, with varying accumulation rates.  Unfortunately, each interval bounded by a hiatus must have more than one section contained within it, and thus, the implementation of sequential hiatuses becomes problematic and highly site-specific since it depends on sampling interval and the length of the pre-/post-settlement sequence within the core.

### Section Thickness

```{r, get_thickness, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}

source('R/thick_model.R')
model_plot <- allan_thick()

```

Bacon models were fit with section thicknesses for values of 5, 10, 15 and 20cm for each core (Figure 8).  Optimum section thickness was assigned from the best fit model.  A generalized linear model (GLM) using a gamma family shows a significant relationship between total core length and best-fit section thickness ($F_{1.241}$ = `r round(model_plot$glm$F[2],1)`, p > 0.001).  This relationship indicates that the principle of wider section thicknesses for Bacon models on longer cores holds generally, but, the distribution of best-fit thicknesses (Figure 8) also appears to indicate that many shorter cores are well served by wide section thicknesses.  The GLM accounts for only 26% of total deviance, and should be treated as indicative, not prescriptive.

```{r, plot_thicknessmodel, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, dev = 'svg'}

model_plot[[1]]

```

**Figure 8**. *Bacon section thickness as a function of total core length for best-fit Bacon models on each of the 282 sediment cores with reconstructed age models from the upper Midwestern United States used in this study.  A curve (blue line) fit with a generalized linear model with gamma family indicates the predicted relationship between maximum core depth and the best-fit core thickness (gray shading represents 1 standard error).*

# Discussion

Large scale synthesis of paleoecological records relies on our ability to distinguish synchronous, time transient and asynchronous events at a range of spatial scales.  For this reason chronology has become central to understanding the past operation of earth system processes [@harrison2016geochronology].  Standardized age models, across data sets within a synthesis project reduces a significant source of variability, and, using Bayesian approaches, can improve the ability of researcher to take temporal uncertainty into account when assessing the influence of climatic and biotic factors in driving vegetation change on the landscape.  Our approach in this paper, and the associated analysis in Dawson et al. [@dawson2016quantifying] and elsewhere has shown what is possible for large-scale sysntehsis projects, and also, we hope, can serve as a roadmap for future endevours.

Databases such as Neotoma provide a resource that can leverage aggregate data to improve individual results.  The finding of and prescription for accumulation rate inflections associated with the pre-/post-settlement horizon provide one example.  The expert elicitation exercise has resulted in age models with settlement horizons of more recent age that originally assigned by investigators.  This
might imply more rapid vegetation change in the post-settlement era, and may change interpretations of vegetation-climate relationships in this era as temporal uncertainty has also increased, as evidenced by our assessment of ^210^Pb dates.

A challenge for paleoecologists is that site level effects can often play a significant role in sediment accumulation, pollen accumulation and the interpretation of model results.  When working with aggregate data it is possible to lose sight of these site level effects, and the potential benefits of doing batch runs with the latest software might make a researcher overlook the peculiarities associated with individual records.  Knowing the data well and taking time to examine results in the context of the original publications is critical to evaluating the quality of a record.  In this exercise a number of records were rejected, a number of records had modifications made and some peculiar records were retained based on documetation within the primary literature.  There is no alternative to knowing your data well.

## Modelling Recommendations

This work has highlighted a number of future opportunities for age-depth model development.  The last decade has seen rapid improvements in modeling software and our understanding of the limitations of chronologies [@trachsel2017all;@telfordfirst], but work remains.  This work can be broadly categorized into parameters, processes and products:

### Parameters

Most modern age-modeling software provides for the use of only a single uncertainty model, generally normally distributed error.  This is at odds with certain age constraints, such as the "modern" sample, which has an absolutely known age, or the pre-settlement horizon (based on the PLS) which provides a certain "older" date, but an uncertain younger boundary, and as such reflects a truncated distribution. Providing a broader range of uncertainty distributions could improve modelling of certain features used as chronological controls in age models.

Our work and the work of others has indicated the variability of certain key parameters through time.  Along with accumulation rates, memory parameters may also change through time, as sediment source changes or as a result of long term changes in precipitation variability evident in regional climate reconstructions [REF].  Accomodating these changes currently requires the use of a zero-length hiatus in Bacon, but allowing time/depth varying parameters using priors drawn from (for example) Neotoma or the Europrean Pollen Database [REF] would provide significantly more control for the models.

Given the large number of parameters required to fit models it is only natural that, at times, changes in parameter value can have unintuitive changes in model performance or fit.  Changing memory parameters should increase or decrease the flexibility of models, but section thickness can also influence flexibility, and these may have interacting effects.  By allowing users to pass vectors of values for each parameter it may be possible to produce a "landscape" of fits with a single call, from which the best fit model may be selected.

## Process

Sedimentation is a physical process that is driven by climatic factors, changes in sediment source, changes in basin size or shape, and autotrophic effects in the case of palustrine environments.  In addition, apparent sedimentation may be affected by the process of coring (as a result of compaction) and in generating composite cores when multiple drives are required to collect a complete sedimentary sequence.  When composite cores are used the information relating to how the cores were spliced together is rarely included, but can introduce depth uncertainty in the composite core.  Work on assessing fit is being undertaken with Corelyzer and indep within the ODP program, but this information is not yet being introduced into age-depth modelling software.

External influcences on sedimentation rate may appear in secondary proxies, for example, grain size or taxonomic composition in peat sequences.  Currently there is no ability to use this information to constrain age model accumulation or flexibility.  Providing these secondary controls would add additional complexity to age models, but could improve overall model fit, particularly as XRF and other associated measurements become more commonplace.

### Product

The age model, as a component of paleoecological studies, is increasing critical, as the use of paleoecological records expands.  As such, the decisions that go into age-model construction must be preserved in a codified manner.  The development of widely accepted provenance standards (e.g., WC3's PROV), and their adoption and promotion within large institutions [REF] mean that this information can be preserved, but adoption of these standards remains a challenge.  Community support for standards, and the development and continued participation within research coordination networks such as C4P or EarthRates are then critical for developing these tools as community standards.  Codifying these choices and examining them in aggregate can also help reduce the requirements for oversight as the volume of information can provide us with a method by which we can quantify the "art" of age-depth modeling.  For example, as the IntCal curve changes, Bacon models that have been preserved in Neotoma with their full suite of parameters may simply be re-run, and clearly attributed as more recent versions.  This was a goal of the INQUA Age Modelling meeting [@grimm2014working], but its implementation remains challenging.

| Challenge | Solution |
| ------------------- | -------------------------------------- |
| Parameter ||
| Age uncertainties vary in the shape of their distributions. |  Allow uncertainty models to include other distributions, including truncated distributions. |
| Accumulation rates change through time but priors are fixed. | The current solution is to use 'zero-length' hiatuses, which serve to reset the accumulation rate, but allowing models of change based on empirical studies would improve fit. |
| Parameter values can have significant, but unintuitive effects on model fit   | Allowing models to accept vectors of possible values may improve fit and provide a better understanding of the interacting relationships between model parameters.   |
| Process ||
| Uncertainty in depth is not represented | Estimates of overlap within composite cores, based on correlation from geochemical or visual cues may be included |
| Models don't use secondary information. | Secondary parameters may be used to indicate rapid changes in memory, or accumulation. |
| Product ||
| Age models require significant oversight |  Codify certain decisions, provide testing suites for fit parameters. |
| Ensuring that model outputs are clear and the decisions made about parameters are justified.  | Improved uptake of reproducible workflows and use of provenance tools. |

## Future development

The tools and techniques for generating age models continues to increase.  BChron [ref], Bacon STAN [ref], CSciBox (url: [https://www.cs.colorado.edu/~lizb/cscience.html](), OxCal [ref] all exist as alternative tools, and geochronR exists as a resource for managing age models in the R programming environment.  It is unlikely the community will select a single resource for modeling chronologies, but the adoption of certain standards across the paleogeosciences can help improve interoperability of data, and ultimately, lead to improvements in our overall understanding of many of the exciting research questions that drive us as a community and as individuals.

# Conclusions

We know lots now that we didn't know before.  If you read carefully you would too.

# Acknowledgements

This work is a contribution of the PalEON Project (http://paleon-project.org).  SJG would like to thank support from NSG, CEG and AMG.

# References
