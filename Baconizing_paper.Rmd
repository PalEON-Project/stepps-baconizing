---
title: "Age Models in Large Scale Paleoecological Synthesis"
author:
- affiliation: Mount Royal University
  name: Andria Dawson
- affiliation: University of Wisconsin - Madison
  name: Simon Goring (co-authored)
- affiliation: University of Wisconsin - Madison
  name: John Williams
bibliography: styles/baconizing.bib
output:
  word_document:
    reference_docx: styles/word_template.docx
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    code_folding: show
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    self_contained: yes
    theme: readable
    toc: yes
    toc_float: yes
dev: svg
highlight: tango
keywords: chronology, geochronology, paleoecology, age-models, Bacon, 210Pb, 14C,
  radiocarbon
csl: styles/elsevier-harvard.csl
abstract: |
  Well constructed chronologies are critical to reliable paleoecological inference and any paleoecological analysis must critically assess the underlying data, age-depth models, and their assumptions and parameters used to build the chronologies. Robust broad-scale syntheses of paleoclimatic and paleoecological data usually require the rebuilding of site chronologies, to ensure that inferences are based on current best practices in age-depth modeling and are not confounded by inconsistencies among sites.  The scope and power of these syntheses are facilitated by 1) advances in Bayesian age-depth modeling approaches and 2) continuing growth of data volumes in community curated data resources in paleoecology.  However, understanding of best practices and parameter settings for these age-depth modeling when working with many records is still poorly understood, and data resources often carry legacy chronologies that must be updated.  This paper uses pollen records and associated chronological controls from the Neotoma Paleoecological Database (()[http://neotomadb.org]) to examine the biases and assumptions made in chronology construction on an aggregate dataset. We examine the challenges of missing uncertainty estimates in 210Pb dates, and the use of zero-length hiatuses to account for changing accumulation rates with depth. We also use this exercise to provide prescriptive guidance for chronology construction in the large-scale re-analysis of records.Simple post hoc calibration of interpolated radiocarbon ages results in differences of 31 -- 470 calendar years when compared to newly constructed age models. 
  Age estimates from 210Pb dates are linearly related to Bacon model estimates, but Bacon-calculated uncertainties are approximately twice the reported 210Pb uncertainty. Sediment deposition times (yrs/cm sediment) show an inflection point and associated five-fold decline at approximately 200 years before present. This shift required setting temporally varying priors for sedimentation rate in Bacon, with a prescribed breakpoint at the settlement horizon. Revised age estimates for the EuroAmerican settlement horizon are, on average, 44 years younger in the new Bacon models than in prior models. The optimal value forsection thickness tends to increases total core length.
  Caution must still be exercised with age-depth modeling; even with advances in software and data resources, accurate age models require expert judgment and human supervision.  One key need is to better capture and store age-depth model metadata and parameter settings in community data resources, to improve reproducibility and to allow quicker regeneration and updating of these age models as parameterizations and models improve. Other recommendations include the addition of additional kinds of uncertainty models in age-depth models, allowing priors and parameters to evolve through time, extension of models to enable fuller parameter space exploration and optimization, integrating of physical sedimentological data into age-depth models, and the better integration of age-depth models and community curated data resources through shared adoption of common standards.

---

# Introduction

```{r load_libraries, echo=FALSE, message=FALSE, warnings = FALSE, results='hide'}

library(purrr, verbose = FALSE)
library(purrrlyr, verbose = FALSE)
library(ggplot2, verbose = FALSE)
library(neotoma, verbose = FALSE)
library(plyr, verbose = FALSE)
library(reshape2, verbose = FALSE)
library(mgcv, verbose = FALSE)
library(Bchron, verbose = FALSE)
library(viridis, verbose = FALSE)
library(dplyr, verbose = FALSE)
library(readr, verbose = FALSE)
library(gridExtra, verbose = FALSE)

version <- 0.1

```

Advances in paleoecoinformatics are continually improving our ability to store and process large numbers of paleoecological records [@brewer2012paleoecoinformatics;@uhen2013card;@williams2017neotoma], which enables the use of increasingly large data networks to study ecological and climatic processes operating at large spatial scales and at timescales inaccessible to modern observational data [@marlon2017climatic;@bothe2015continental;@abram2016early]. Paleoecological and paleoclimatic databases streamline this process by providing the means to store, curate, and query structured data across specified spatial and temporal bounds. Examples of these databases include NOAA-Paleoclimatology (https://www.ncdc.noaa.gov/data-access/paleoclimatology-data)[], Linked Earth (http://linked.earth/)[], and the Neotoma Paleoecology Database [@williams2017neotoma; URL: http://neotomadb.org]. Generating accurate and precise age inferences using the best available age controls and state-of-the-art age-depth models is a key challenge for scientists using paleoecological data resources for large-scale data syntheses.  Because of the increasingly large data volumes involved (for example, as of `r Sys.Date()`, Neotoma now includes `r length(neotoma::get_site())`) site-level records, including `r length(neotoma::get_dataset(datasettype="pollen"))` fossil pollen records globally), the central challenge in large-scale regeneration of chronologies is how best to balance speed/automation with careful critical analysis and adjustment of individual age models by experts.  Here we review these issues, assess key sources of uncertainty, and provide examples of solutions developed during our recent data synthesis efforts with the Neotoma Paleoecology Database and the Paleoecological Observatory Network (PalEON).

An emerging practice in paleoecological and paleoclimatic data resources is to store both the original proxy measurements by depth (*e.g.*, the stratigraphic position of vertebrate specimens or micropaleontological assemblages) and the associated age controls, so that age estimates for the proxy measurements can be regenerated as age-depth models and geochronological parameterizations improve [@grimm2014working;@blois2011methodological;@giesecke2014towards].  The chronology for a given record comprises three distinct elements: the age-depth model used for a given paleoecological or paleoclimatic record, the age controls that constrain that age model, and the resultant age estimates for all stratigraphic depths or intervals in that record [@mckay2015linked;@grimm2008neotoma]. Robust chronologies require accurate and precise estimates of absolute ages from radiometric and other age controls, well supported conversion from radiocarbon years to calendar years, age-depth models that make use of prior knowledge [*e.g.* Steno's Law of stratigraphic superposition and rates of sediment deposition and accumulation, @goring2012deposition], and a clear quantification of uncertainty that accounts for both measurement and process uncertainty.  

The accuracy and precision of these chronologies are fundamental to paleoecological interpretation, and indeed, geochronology and age models are central to our understanding of the evolution of the earth system [@harrison2016geochronology]. Quaternary scientists are, of course, well aware of the fundamental importance of chronologies, and various papers have explored these issues and the interpretive challenges that can arise from them [@grimm2009magnitude;@liu2012temporal;@trachsel2017all;@blaauw2012out].

Data resources such as Neotoma or LinkedEarth/LiPD can store multiple chronologies for a given record.  Because updating age-depth models takes time and expertise, the stored chronologies do not always conform to the current state of the art in chronology construction.  These legacy age models may have various issues, e.g. 1) no conversion from radiocarbon years to calendar years, 2) conversions that are based on outdated radiocarbon calibration curves, 3) a lack of robust uncertainty quantification, 4) inconsistencies among sites in reconstruction methods, resulting in potential biases. One shortcut workaround used in Neotoma for its legacy chronologies expressed in radiocarbon years is a *post hoc* conversion of individual age estimates to calendar years.  This practice is sufficient for first-pass queries to discover and retrieve data by temporal bounds, however this is insufficient for research-quality data syntheses since it produces systematic offsets on the order of hundreds of years (Fig. 1).

```{r get_cores, echo=FALSE, message=FALSE, results='hide', warning=FALSE}

source('R/getNAcores.R')

all_downloads <- north_american_cores(version)

if (paste0('pubs_v', version, '.rds') %in% list.files('data/output')) {
  pubs <- readRDS(paste0('data/output/pubs_v', version, '.rds'))
} else {
  pubs <- neotoma::get_publication(all_downloads)
  saveRDS(pubs, file = paste0('data/output/pubs_v', version, '.rds'))
}

meta.table <- pubs %>% 
  map(function(x){
    metas <- do.call(rbind.data.frame, 
                     lapply(x, '[[', 'meta'))
    output <- metas[which.min(metas$year),]
    if(nrow(output) == 0) {
      output <- data.frame(id = NA, 
                           pub.type = NA, 
                           year = NA,
                           citation = NA)}
    return(output)}) %>% 
  bind_rows

chron <- sapply(all_downloads, function(x){
  types <- sapply(x$chronologies, function(x) x$age.type[1])
  age_types <- types[which.min(match(types, c("Calendar years BP",
                                              "Varve years BP",
                                              "Calibrated radiocarbon years BP",
                                              "Radiocarbon years BP")))]
  if(length(age_types) == 0) { age_types <- NA }
  return(age_types)})

meta.table$chron <- NA
meta.table$chron <- chron

source('R/section_1_scripts.R')

```

<object type="image/svg+xml" data="figures/agediff_plot_final.svg" style="width: 100%;">
Differences between age models and direct recalibration.
  <!-- fallback image in CSS -->
</object>

**Figure 1**. *Diagram of two methods for generating calibrated dates from radiocarbon age-models.  (a) Each age (in radiocarbon years) is calibrated directly (red arrows) from an existing model (black dashed line) or (b) the age models is reconstructed using linear interpolatation (red dashed line) from calibrated radiocarbon ages (red line).  Negative values indicate that recalibration of interpolated ^14^C dates followed by linear interpolation of calibrated ages provides systematically older ages than direct recalibration of interpolated ages. (d) Using the chronological controls of the default age model from Neotoma records, calibrate chronological controls reported in radiocarbon years, and then use linear interpolation.  Any records with age-reversals were rejected to simplify this illustrative example.*

Hence, an early step in any large-scale data synthesis is to review the extant chronologies associated with the records at hand and update them as needed. Ideally, these updated chronologies should be placed back into the archival data resources, to maximize comparability of results across studies, reproducibility, and reuse by other researchers. Recent examples of large-scale paleo-synthesis analysis have focused on standardizing age models for records within a database  and establishing regional benchmarks . Age model standardization in Europe relied on the Clam software , as did efforts with the North American Pollen Database [@blois2011methodological]. 

At the same time, Bayesian approaches are becoming the new standard in age-depth modeling, because of their ability to return robust uncertainty estimates, and to incorporate knowledge about system processes and prior knowlege into the sedimentation models [*e.g.*, @blaauw2011flexible;@ramsey2013recent;@parnell2016package]. Although many site-level papers have adopted Bayesian methods, few large scale syntheses have done so [e.g., the ACER database used Clam: @sanchez2017acer].

A number of issues arise during the large scale generation of many chronologies. First, building an age-depth model requires expert judgement: there are enough complexities in sedment stratigraphies and geochronological data that fully automated plug-and-play solutions do not yet have a high probability of returning accurate age inference.  At the same time, the subjectivity and flexibility inherent to age-depth modeling may introduce additional "researcher degrees of freedom" that can lead to greater rates of false-positive relationships in paleoecological research [-@blaauw2012out]. Bacon, for example, contains various parameters, priors, and settings (e.g., sediment accumulation rates) that may be unknown, or complex to implement at large spatial or temporal scales [@sanchez2017acer].

Reliable sediment age estimates require accurate dating of high quality stratigraphic control points, well supported conversion from radiometric (often radiocarbon) to calendar years, robust and reliable models that mehcanistically or empirically represent the underlying process of sediment deposition and accumulation, and clear quantification of uncertainty that is able to take into account measurement and process uncertainty.

Chronology construction is dependant on the availability of dated stratigraphic control points from a sediment core. These control points may be geochronological (dated material), geostratigraphic (*e.g.*, the "modern" core top), or biostratigraphic (changes in pollen assemblages associated with dated changes on the landscape). Geochronological ages are often uncertain due to analytical errors in laboratory radiocarbon dating processes [@ward1978procedures], the calibration of certain age types [*e.g.*, radiocarbon to calendar year calibration; @reimer2013selection], and potential differences between the ages of dated material and the age of deposition of the sediment matrix [@blois2011methodological]. Geostratigrahic markers may have fewer sources of uncertainty, the core top age is assumed to be the year of sampling and tephras are often widespread which provides broader support for dated ages, however, sedimentary mixing may introduce uncertainty. Biostratigraphic control points are determined by the examination (usually visual) of changes in biological proxy assemblages throughout a core. However, time series of proxy data are often noisy, and in practice identifying changes in composition is both difficult and subjective [@dawson2016quantifying]. There remains uncertainty in the identification of compositional shifts related to the chronological/stratigraphic sampling density [@liu2012temporal], and, in the case of landscape-scale phenomena such as the mid-Holocene Hemlock decline [@bennett2002determining;@davis1981outbreaks], the need to assign temporal bounds to the landscape-scale phenomenon that caused the compositional shift.

Chronologies are approximations of physical processes that lead to variablility in sediment accumulation rates within depositional basins.  While methods such as Bacon [@blaauw2011flexible] include an autocorrelation term intended to reflect physical processes, they do not explicitly attempt to model the physical process of sedimentation. Theoretical work extends our understanding of sedimentation within lake basins [@bennett2016interpretation], describing broad relationships between basin shape and long term patterns of accumulation that can improve the mechanistic characterization of sedimentation processes, but broad-scale empirical support would require links between paleoecological resources and bathymetric data that is yet unavailable. Varved lake records can act as controls to validate model precision and accuracy [@trachsel2017all]. Recent work compiling published varved records [@ojala2012characteristics] provides further insight into chronology accuracy. However varved records are often spatially clustered, and are the result of a range of physical, biological and climatic conditions that lead to the possibility that these systems may not reflect broader controls on lacustrine systems without rhythmites, varves or other laminated sediments.

The development of the first radiocarbon calibration curve [IntCal98; @hughen1998intcal98] allowed for the association of radiocarbon years with calendar years, based on material with known ages and associated ^14^C dates. Continued improvement of the calibration curve has resulted in improved chronology development, but chronologies are rarely updated as new curves are generated [@grimm2014working]. Offset and uncertainty are influenced by the method used to re-calibrating legacy models using only radiocarbon years. Comparisons between re-calibrated age models against directly re-calibrated dates highlight two issues: the possibility of systematic offsets resulting in biases in age estimates, and the need for direct intervention and systematic rules-based chronology construction in synthesis work.

```{r, plot.chron.change, fig.width=6, echo=FALSE, message=FALSE, results='hide', warning=FALSE, dev='svg'}

source('R/figures/ageplot_histogram.R')

chronology_type_histogram(meta.table)

```

<object type="image/svg+xml" data="figures/histograms_cleaned.svg">
  Histograms of age model types against year of publication 
  <!-- fallback image in CSS -->
</object>

**Figure 2**.  *Number and type of default chronologies for North American pollen records in the Neotoma Paleoecological Database based on the original year of publication for the dataset.  Within Neotoma the default dataset chronology may be updated by subsequent researchers when this information is provided to a data steward.*

We take the position that developing new age models with recalibrated stratigraphic control points that can account for temporal uncertainty is the ideal approach. However, large scale data analysis across heterogeneous data can be complex, thus the use of harmonized age models, that integrate ideas about the underlying mechanics of the processes linking depth and time become important [@blaauw2011flexible]. The Neotoma database [@grimm2008neotoma] contains `r length(get_dataset(datasettype="pollen"))` global pollen records that can be used for paleoecological analysis.  These records have been obtained from publications that span a time period from  `r min(meta.table$year, na.rm=TRUE)` to `r max(meta.table$year, na.rm=TRUE)`, with more than half the records coming from before 1983.

This paper builds on work from Dawson *et al*. [@dawson2016quantifying], which rebuilt existing age models using the Bacon age-depth model, by explicitly detailing the key challenges posed by generating a large number of Bayesian age models and quantifying the effects of decisions on aggregate outcomes. We outline the decision making process around age model parameter selection, summarize differences between the original chronologies and the new chronologies, highlight differences between chronologies generated using two age model types and highlight best practices for chronologies within large-scale synthesis.  We also identify limitations of current methods, that could serve to improve chronology construction in the future.

# Key Issues in Generating Age Models from Neotoma

## Use of *posthoc* calibration of individual ages in radiocarbon chronologies

Many North American pollen records in Neotoma still record chronologies using only radiocarbon years (Figure 2). The transition from age models using only radiocarbon years to those with calibrated radiocarbon years within Neotoma is dramatic.  The final radiocarbon model appears to be from 1998, following this we see no more radiocarbon models.  Along with this transition, a second transition from simple linear models to more complex models using flexible Bayesian methods exists, although this is not shown.

An illustrative model for this paper generally could be: When using records with chronologies in uncalibrated radiocarbon years, should we calibrate radiocarbon dates, generate age models *de novo*, or ignore the records altogether?  This question is illustrative since it helps illustrate the problems associated with methodological assumptions, and varying data sources in paleoecological synthesis work.

While not the preferred method, direct recalibration of interpolated ages does occur within the Neotoma ecosystem. For example, the temporal search function within the Neotoma Explorer (http://apps.neotomadb.org/explorer), Tilia (http://tiliait.org) and the Neotoma API (http://api.neotomadb.org) all use a lookup table that directly recalibrates ages in radiocarbon years. However, this process results in systematic biases in both synthetic data and in the actual Neotoma data (Figure 1).

We take the position that develping new age models based on recalibrated stratigraphic control points that account for uncertainty is the ideal approach. However, large scale data analysis across heterogeneous data can be complex, thus the use of harmonized age models, that integrate ideas about the underlying mechanics of the processes linking depth and time become important [@blaauw2011flexible]. Developments since Blois *et al*. [-@blois2011methodological], and workshops such as the PAGES-sponsored Age Models, Chronologies, and Databases Workshop [@grimm2014working] have begun the process of outlining the methods required to undertake large-scale efforts to re-build age models.

## Choice of Age-Model Type and Software

Estimating age-depth relationships and uncertainties requires that decisions be made about the type of age modeling approach, choice of software implementation, suitable chronological controls and model parameters [@blois2011methodological;@grimm2014working;@giesecke2014towards]. We here pursue Bayesian approaches for their ability to flexibily estimate the ages of pollen samples with robust uncertainty. Several Bayesian age modelling software packages exist, including Bacon, Bcal, Bchron, and Oxcal. We use Bacon because it 1) implements a Bayesian framework that estimates sample age posteriors, 2) is widely used by paloecologists, and 3) accounts for some of the key components in the sediment deposition and accumulation process. Using Bacon to estimate age-depth relationships requires the specification of: 1) chronological controls, which include radiometric dates, biostratigraphic markers, and their uncertainty, 2) priors on the accumulation rate and memory, 3) values for the resolution and structure of divisions within the sediment core (section thicknesses and hiatuses). To ensure a consistent methodology among sediment core in the UMW domain, we developed a set of standard decisions that are described below.

## Choice of Age Controls and Types

Neotoma contains `r nrow(neotoma::get_table('ChronControlTypes'))` different chronology control types, including various pollen stratigraphic controls, radiocarbon and other isotopic dates and marine isotope stages.  Since this analysis was focused primarily on sedimentary pollen records, most of the chronological controls used reflect more recent control types, including ^210^Pb, ^137^Cs, and ^14^C dates, as well as biostratigraphic dates associated with events of known or inferred dates, such as historical fires and the *Tsuga* decline.  Core tops are often used, and have highly certain dates, however, several older records use interpolated dates for the core top when sediment was lost in the coring process.  These cores should be treated with caution, and this is often noted in the core description.

```{r, results = 'hide', echo = FALSE, warning=FALSE}
if('controls.RDS' %in% list.files('data/output/rds_files/')) {
  controls <- readRDS('data/output/rds_files/controls.RDS')
} else {
  controls <- lapply(all_downloads, function(x)try(get_chroncontrol(x)))
  for(i in length(controls):1){if('try-error' %in% class(controls[[i]])) {controls[[i]] <- NULL }}
  saveRDS(controls, 'data/output/rds_files/controls.RDS')
}

control_type <- controls %>% 
  map(function(x)data.frame(control = as.character(x$chron.control$control.type))) %>% 
  bind_rows() %>% 
  group_by(control) %>% 
  summarise(count = n())

knitr::kable(control_type)
```

## Chronological Uncertainty

Methods of understanding an managing radiocarbon uncertainty are well established, however the use of ^210^Pb and biostratigraphic dates are less well understood in the context of sedimentary archives, particularly in large scale synthesis work. Biostratigraphic events often require indentification by the researcher and may be time-transitive while uncertainties associated with ^210^Pb may appear to narrow to properly model ages using the Bacon software.

### Dealing with zero-value ^210^Pb errors

Bacon requires that all chronological markers be associated with defined errors. Historically, some ^210^Pb data entered into Neotoma were entered without error reporting.  Of the 398 ^210^Pb age controls in Neotoma, 148 have no error reported (Figure 3).  Binford [-@binford1990calculation] reports *"Ninety-five per cent confidence intervals range from about 1 -- 2 years at 10 years of age, 10 -- 20 at 100 years, and 80 -- 90 at 150 years old."*  Using this assessment we fit a smooth linear function to assign 95% confidence intervals for all ^210^Pb dates with missing uncertainty data.  These confidence intervals were then divided by 2 (and rounded up to the nearest integer) to obtain standard deviations to be used in the Bacon model (Figure 3). 

```{r, echo=FALSE, message=FALSE, results='hide', warning=FALSE, dev='svg'}

all_geochron <- readRDS(paste0('data/output/all_geochron_v', version, '.rds'))

source('R/lead_plotting.R')

lead_plots(all_geochron)

```

**Figure 3.** *Reported uncertainty for ^210^Pb dates within Neotoma, by age, with the total count of unassigned samples by age bin (lower).  Estimated uncertainty, using a model relating reported ^210^Pb uncertainty to age, is then assigned to all ^210^Pb dates with unreported uncertainty. Present is defined as radiocarbon present, or 1950 CE.* 

### Expert elicitation exercise to identify biostratigraphic events

Biostratigraphic events have been used extensively as chronological controls within age models, but are often poorly constrained with respect to the identification of the "rise" event. The *Ambrosia* rise is a well known phenomenon in the eastern United States, and elsewhere, that appears to be contemporaneous with Euro-American settlement, land clearance and the initiation of intensive agriculture in the region [@McAndrews1968;@mcandrews1988human]. In the Upper Midwest, significant increases in *Ambrosia*, *Rumex*, and/or Poaceae are typically coincident with the settlement horizon. Because of this feature it is possible to establish a biostratigraphic chronological control. When the rise can be identified visually it can be used as a stratigraphic marker with an age defined as either immediately preceeding (when the "pre-settlement" sample is used), or immedately after.  The pre-settlement sample has been identified for many of the fossil pollen cores in the Neotoma database.  To reduce expert bias in the selection of the 'pre-settlement' sample we asked a team of experts to identify pre-settlement samples based on pollen diagrams depicting proportional changes as a function of depth for key indicator species and the ten most abundant arboreal taxa, with no temporal scale.  More details of this procedure are available in Dawson et al. [-@dawson2016quantifying] and in Kujawa et al. [@kujawa2016theeffect].

The elicitation exercise provides an independently assesed estimate of biostratigraphic change in the pollen records that can be used as a biostratigraphic marker. Gridded Public Land Survey (PLS) datasets in this region provide a historical record of the minimum date of presence for legacy forests [@goring2016novel]. Goring et al. [-@goring2016novel] use historical records of forest cover from the 1820s - 1903 to provide estimates of forest cover prior to EuroAmerican settlement.  The gridded datasets associated with the paper also provides the PLS sampling year; we subtract 50 from the value for grid cells in which sites fall to assign a "pre-settlement" period for each core, with an age uncertainty (stanadard deviation) of 50 years.  Ideally, methods would include asymmetric uncertainty, since, in the case of the settlement horizon, we know the minimum age of, but not the most recent age for potential settlement.

Five cores were located within grid cells without digitized PLS data. In these cases, we look to the surrounding grid cells to estimate the maximum sampling year. One site in the Upper Peninsula of Michigan was assigned a maximum sampling year of 1860. The four remaining cores were in the Lower Peninsula of Michigan, and were assigned a maximum year of sampling of 1840.

## Bacon Settings

### Accumulation Rate Priors

Bacon provides a default accumulation rate for all depths based on Goring *et al*. [-@goring2012deposition]'s survey of Holocene accumulation rates in eastern North America.  Empirical age-depth curves suggest that the age-depth relationship is non-linear, as a result of lower sediment compaction in upper sediments [@goring2012deposition], basin shape [@bennett2016interpretation] and patterns of deposition and sediment transport operating on longer time scales [@goring2012deposition;@webb1988rates] and acceleration of erosion rates during the Anthropocene [REF]. Narrowing the geographic and temporal range for the target reconstructions made it neccessary to re-asses mean accumulation rates, focusing on a narrower temporal and spatial window.  Regionally, there is evidence that after the time of settlement, sediment accumulation is much faster (fewer years per cm of sediment accumulation) than before settlement (more years per cm of sediment accumulation), although bulk density may decrease.

Many sites show an inflection point in sedimentation rates at or around the Anthropocene horizon in the Upper Midwest.  This is likely a combination of both decreased compation in upper sediments and changes in land use and erosion. To generate accumulation rate priors for the Upper Midwest we estimated accumulation rates by pooling the mean age and change in depth for adjacent chronological markers. These accumulation rates were pooled into "modern" and "pre-settlement" groups. For each group a mean equal to the empirical mean of accumulation rates for that time period, and a variance equal to double the empirical variance from the grouped accumulation rates was calculated.  Doubling the empirical variance represents a conservative approach to modeling uncertainty, in that it accounts for additional variability in accumulation rates that was not observed in the data.

The modern accumulation rate prior, from more recent sedimentation rates was assigned to the post-settlement portion of cores with "settlement" horizons assigned through the Expert Elicitation. The 2K prior was assigned for the pre-settlement portion of the cores, and for the entirety of cores without identified "pre-settlement" horizons.  Cores with two priors were assigned an "instantaneous" (10yr) hiatus at the pre-settlement sample depth.

### Determining section thickness

Bacon works by diving a core into sections whose lengths are determined by the user [@blaauw2011flexible]. Sections are the atomic unit of a Bacon model, and various parameters are fit within each section.  In particular, the `memory` parameter defines the flexibility of the accumulation rates between adjacent sections, thus a model with large section thicknesses will be, by virtue of having fewer overall sections, less flexible. However, narrow section thicknesses result in very large run-times, and may show other, unanticipated problems associated with finding fits in multi-parameter space.

Dividing a core into sections is an approximation of time discretization [REF - Andria?], abstracted through the process of deposition.  Consistent section widths across all cores (and thus consistent discretization) should be a goal. However, the internal unmeasurable variability in sedimentation rates, and uncertainty in the age-depth models themselves, makes it difficult to implementat consistent section thicknesses.  To provide prescriptive widths, but still allow flexibility as needed in the Bacon modelling, each core was run with widths of 5, 10, 15 and 20cm.  Model fit was assessed visually, and the best fit model was subsequently chosen to be the default calibrated age model for that core.

# Results

## Age Controls

### ^210^Pb Errors

```{r, lead_comparison, results = 'hide', echo = FALSE, warning=FALSE, dev='svg'}

source('R/lead_ages.R')
comp_doc <- compare_lead()

model  <- lm(I(1950 - bacon_age1) ~ age, data = comp_doc[[2]])
uncert <- lm(bacon_error1 ~ e.older, data = comp_doc[[2]])

comp_doc[[1]]
```

**Figure 4**. *Reported and estimated ^210^Pb ages from the Bacon models show strong accordance, for the most part.  Uncertainties within the constructed Bacon models at the depths of the ^210^Pb samples show higher uncertainty from Bacon models than from the original uncertainty models. The autoregressive nature of the Bacon model, the influence of the memory parameter, and the nature of the Bayesian model itself mean that uncertainty is propagated through the core, and as such low uncertainty in the individual date (x-axis) does not result in low Bacon uncertainty.*

Bacon ages associated with ^210^Pb ages show a linear relationship, with the Bacon ages older than the reposted ^210^Pb ages (slope = `r round(model$coefficients[2], 2)`, p < 0.01; Figure 4).  Uncertainty estimates for the Bacon chronologies are also consistently larger than the ^210^Pb error estimates for the point samples and the difference increases with depth (slope = `r round(uncert$coefficients[2],2)`, p < 0.01).

Only three lakes in the region had age models that included ^210^Pb dates and reported estimates of uncertainty for the chronology. Bacon estimates for these records show higher model uncertainty than the original age model, but the difference in modeled uncertainty is site-specific (Figure 5).  For Crooked Lake [@brugam1997holocene] the uncertainty difference is high and consistent with depth/age (triangles, Figure 5), while for Fish Lake [@umbanhowar2004interaction] the difference is small (squares; Figure 5).  Brown's Bay shows the highest uncertainty, but only at the deepest depth (circles; Figure 5).

```{r, error_by_age, results = 'hide', echo = FALSE, warning=FALSE}

lead_out <- comp_doc[[2]]

drop_outlier <- lead_out %>% 
  filter(bacon_error1 < 200) %>% 
  select(site.name, age, bacon_error1, e.older) %>% 
  na.omit() %>% 
  arrange(site.name, age)

lead_comp_plot <- ggplot(data = drop_outlier) + 
  geom_point(aes(x = age, y = bacon_error1, shape = site.name), 
             alpha = 0.7, size = 3) +
  geom_point(aes(x = age, y = e.older, shape = site.name), 
             color = 'red', alpha = 0.7, size = 3) +
  geom_ribbon(aes(x = age, ymin = e.older, ymax= bacon_error1, group = site.name),
              alpha = 0.1, color = alpha(colour = 'black', 0.2)) +
  geom_segment(aes(x = age, xend = age, 
                   y = bacon_error1, yend = e.older), alpha = 0.3) +
  scale_x_continuous(limits=c(-50, 100)) +
  scale_y_continuous(limits=c(0, 80)) +
  coord_equal(expand = c(0,0)) +
  xlab('Years Before Present') +
  ylab('Model Uncertainty') +
  annotate("text", x = -25, y = 80, label = "Bacon Uncertainty") +
  annotate("text", x = -25, y = 75, label = "210Pb Uncertainty", color = "red") +
  theme_bw() +
    theme(axis.title.x = element_text(family = 'serif', 
                                      face = 'bold.italic', 
                                      size = 18),
          axis.title.y = element_text(family = 'serif', 
                                      face = 'bold.italic', 
                                      size = 18),
          axis.ticks = element_blank(),
          axis.text.x = element_text(family = 'serif', 
                                     face = 'italic', 
                                     size = 14),
          axis.text.y = element_text(family = 'serif', 
                                     face = 'italic', 
                                     size = 14))

ggsave(filename = 'figures/lead_comp_plot.svg', plot = lead_comp_plot, width = 8, height = 5)

```

<object type="image/svg+xml" data="figures/lead_comp_final.svg">
  Comparison of 210Pb dates measured and reconstructed error at those depths. 
</object>

**Figure 5**. *For the three cores within the region with both reported ^210^Pb and reconstructed Bacon uncertainties, the latter are consistently higher, although the magnitude of the difference varies by record.  Fish Lake shows the smallest difference, while reconstructed ages for Brown's Bay shows the largest difference.*

### Expert Elicitation

```{r, elecitation_plot, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
source('R/elicitation_plot.R')

elicitation_plots <- expert_elicitation()

```

Original modeled ages were older than the biostratigraphic ages assigned based on the dates for the Public Land Survey in the vicinity of the lakes of interest.  Of the 163 records with assigned settlement ages from the elicitation exercise, 55 of the original age models have a modeled ages at the assigned settlement horizon that are younger than 1800 CE, while only 17 of the Bacon models have a modeled age for the settlement horizon younger than 1800 CE.  Bacon models showed better affinity to the assigned biostratigraphic dates based on expert identification of the *Ambrosia* rise in the region.  This age differential means that Bacon models are, on average, 44 years younger at the settlement horizon than the original models, which would result in greater apparent rates of change for pollen in near-modern sediments.

```{r, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, dev = 'svg'}
elicitation_plots[[1]]
```

**Figure 6**. *The relationship between assigned 'settlement' horizons, based on Public Land Survey records and the expert elicitation exercise, and the modeled age of the horizon within the final age-depth model for each individual record.  The settlement horizon plotted against the modeled age within the original age model (left); and against the newer Bacon model (right).  A best fit linear regression is plotted in blue, with a 1SE envelope in gray. The negative relationship in the original models may reflect an over estimation of sedimentation rates in upper sediments as a result of linear age-model fits. Higher variability in the orginal relationship may represent greater inter-researcher variability in settlement horizon estimation.*

Linear models relating assigned settlement ages to modeled ages for the original age models show a negative slope (Figure 6). This may be because many of the original linear models over-estimate sedimentation rates in the upper sediments since they assign an inflection point at the horizon. Higher variability is likely a result of both variability in detecting and assigning the settlement horizon among researchers, but also in the process for assigning the age associated with settlement regionally. Researchers may have used historical documents, oral histories or other records. The use of the standardized PLS dates in te new models reduced this source of variability.

## Bacon Settings

### Accumulation Rate Priors

**Table 1**. *Empirical estimates of accumulation rates in the Upper Midwestern United States from Neotoma records.*

 Group   Mean	   Variance	    Rate	   Shape
------ -------- ----------- -------- -------
Modern	3.02	     5.70	      0.53     1.6
2K	    15.00	     250	      0.06	   0.9


Accumulation rates (*yr/cm* of sediment accumulation) for records in the region show clear differentiation between accumulation rates within the last 200 years and accumulations prior to the last 200 years. Means and variance are presented in **Table 1** along with the Gamma rate and shape parameters. Accumulation rates (the mean *yr cm^-1^* deposition) change dramatically, dropping by five-fold in the modern period.

```{r, get_acc_rates, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, dev = 'svg'}

source('R/acc_rate_priors.r')

plot_acc_rates(accs)

```

**Figure 7**. *Sediment accumulation rates for cores in the study region with samples in the last 2000 years (in calibrated radiocarbon years, with 0 at 1950 CE).  The upper panel shows the accumulation rate in years per cm against the interval midpoint age in calibrated radiocarbon years. The dashed vertical line indicates 1850 CE, the approximate date used to represent major land-use change as a result of EuroAmerican settlement. Note the strong change in accumulation rates between the periods before and after EuroAmerican settlement. The lower panel shows the distribution of accumulation rates for the pre- and post settlement eras, highlighting the difference in central tendency.*

Deposition rates increase with time across the pre-/post-settlement interval (Figure 7a), however Bacon does not support the use of time-dependent priors, except through the use of sequential hiatuses, with varying accumulation rates in adjacent sections. Unfortunately, each interval bounded by a hiatus must have more than one section contained within it, and thus, the implementation of sequential hiatuses becomes problematic and highly site-specific since it depends on sampling interval and the length of the pre- and post-settlement sequence within the core.

### Section Thickness

```{r, get_thickness, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}

source('R/thick_model.R')
model_plot <- allan_thick()

```

Bacon models were fit with section thicknesses for values of 5, 10, 15 and 20cm for each core (Figure 8).  Optimum section thickness was assigned from the best fit model.  A generalized linear model (GLM) using a gamma family shows a significant relationship between total core length and best-fit section thickness ($F_{1.241}$ = `r round(model_plot$glm$F[2],1)`, p > 0.001).  This relationship indicates that the principle of wider section thicknesses for Bacon models on longer cores holds generally, but, the distribution of best-fit thicknesses (Figure 8) also appears to indicate that many shorter cores are well served by wide section thicknesses.  The GLM accounts for only 26% of total deviance, and should be treated as indicative, not prescriptive.

```{r, plot_thicknessmodel, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, dev = 'svg'}

model_plot[[1]]

```

**Figure 8**. *Bacon section thickness as a function of total core length for best-fit Bacon models on each of the 282 sediment cores with reconstructed age models from the upper Midwestern United States used in this study.  A curve (blue line) fit with a generalized linear model with gamma family indicates the predicted relationship between maximum core depth and the best-fit core thickness (gray shading represents 1 standard error).*

# Discussion

Large scale synthesis of paleoecological records relies on our ability to distinguish synchronous, time transient and asynchronous events at a range of spatial scales. For this reason geochronology has become central to understanding the past operation of Earth System processes [@harrison2016geochronology].  Standardized age models, across data-sets reduces a significant source of variability. Coupling this standardization with Bayesian approaches can improve the ability of researcher to take temporal uncertainty into account when assessing the influence of climatic and biotic factors in driving vegetation change on the landscape.  Our approach in this paper, and the associated analysis in Dawson et al. [@dawson2016quantifying] and elsewhere has shown what is possible for large-scale sysntehsis projects, and also, we hope, can serve as a roadmap for future endevours.

Paleoecoinformatic approaches using databases such as Neotoma provide a resource that can leverage aggregate data to improve individual results [@brewer]. The use of accumulation rate inflections associated with the pre-/post-settlement horizon, from database interrogation, and the re-examination of ^210^Pb dates from legacy records provide two examples of the utility of such resources.  The expert elicitation exercise has resulted in age models with settlement horizons of more recent age that originally assigned by investigators.  This might imply more rapid vegetation change in the post-settlement era, and may change interpretations of vegetation-climate relationships in this era as temporal uncertainty has also increased, as evidenced by our assessment of ^210^Pb dates.

Caution must be exercised in undertaking the batch re-calculation of chronologies.  Site-level effects play a significant role in sediment accumulation, presenting a challenge for paleoecologists when examining pollen accumulation and interpreting model results. When working with aggregate data it is possible to lose sight of the importance of site-level effects; the potential benefits of batch runs with newer techniques might make a researcher overlook the peculiarities associated with individual records.  Knowing the data well and taking time to examine results in the context of the original publications is critical to evaluating the quality of a record. In this exercise, a number of records were rejected, a number of records had modifications made and some peculiar records were retained based on documetation within the primary literature.  There is no alternative to knowing your data well.

## Modelling Recommendations

This work has highlighted a number of future opportunities for age-depth model development.  The last decade has seen rapid improvements in modeling software and our understanding of the limitations of chronologies [@trachsel2017all;@telford2004all], but work remains.  This work can be broadly categorized into *Parameters*, *Process* and *Product*:

### Parameters

Most modern age-modeling software provides for the use of only a single uncertainty model, generally normally distributed error.  This is at odds with certain age constraints, such as the "modern" sample, which has an absolutely known age, or the pre-settlement horizon (based on the PLS) which provides a fixed "older" date, but an uncertain younger boundary, and as such reflects a truncated distribution. Providing a broader range of uncertainty distributions could improve modelling of certain features used as chronological controls in age models.

Our work and the work of others [@goring2012deposition;bennett2016interpretation;@webb1988rates] has indicated the variability of certain key sedimentation rate parameters through time. The autocorrelation of accumulation rates (in Bacon, the memory parameter) may also change through time, as the sediment source changes, or, as a result of long term changes in precipitation variability evident in regional climate reconstructions [regional papers. . . ].  Currently, accomodating changes in any parameter through time requires the use of a zero-length hiatus in Bacon, and may be impossible with other software. Allowing time/depth varying parameters using priors drawn from (for example) Neotoma or the Europrean Pollen Database [@brewer2016late] would provide significantly more control for chronology models.

Given the number of parameters required to fit models, and their interactions, it is only natural that, at times, changes in parameter values can have unintuitive changes in model performance or fit. Changing memory parameters should increase or decrease the flexibility of models, but section thickness can also influence flexibility, and these may have interacting effects.  By allowing users to pass vectors of values for parameters it may be possible to produce a "landscape" of fits with a single call, from which the best-fit model may be selected.

## Process

Sedimentation is a physical process that is driven by climatic factors, changes in sediment source, changes in basin size or shape, and autotrophic effects. Artifacts in the modeling of sedimentation rates may be affected by the process of coring (as a result of compaction) and in generating composite cores when multiple drives are used to collect a complete sedimentary sequence. Information relating the way multiple drives are spliced is rarely included in data records, but can introduce depth uncertainty in the composite core. Work on fit assessment is being undertaken with Corelyzer [ref] and independently within the ODP program [ref], but this information is not yet being introduced into age-depth modelling software.

External influcences on sedimentation rate may appear in secondary proxies, for example, grain size [ref] or taxonomic composition in peat sequences [ref].  Currently there is no ability to use this information to constrain age model accumulation or flexibility.  Providing these secondary controls would add additional complexity to age models, but could improve overall model fit, particularly as XRF and other associated measurements become more commonplace and accessible.

### Product

The age model, as a component of paleoecological studies, is increasing critical, as the use of paleoecological records expands. As such, the decisions that go into age-model construction must be preserved in a codified manner. The development of widely accepted provenance standards (e.g., W3C PROV), and their adoption and promotion within large institutions [REF] mean that this information can be preserved, but adoption of these standards remains a challenge.  Community support for standards, and the development and continued participation within distributed research networks such as the Earth Sciences Information Partnership, Cyber4Paleo or EarthRates, are critical for developing these tools as community standards.  Codifying these choices and examining them in aggregate can also help reduce the requirements for oversight as the volume of information can provide us with a method by which we can quantify the "art" of age-depth modeling.  For example, as the IntCal curve changes, Bacon models that have been preserved in Neotoma with their full suite of parameters may simply be re-run, and clearly attributed as more recent versions.  This was a goal of the INQUA Age Modelling meeting [@grimm2014working], but its implementation remains challenging.

**Table 2**. *A summary of challenges and solutions for the process of age-modelling within the paleoecological community.*

| Challenge | Solution |
| ------------------- | -------------------------------------- |
| **Parameter** ||
| Age uncertainties vary in the shape of their distributions. |  Allow uncertainty models to include other distributions, including truncated distributions. |
| Accumulation rates change through time but priors are fixed. | The current solution is to use 'zero-length' hiatuses, which serve to reset the accumulation rate, but allowing models of change based on empirical studies would improve fit. |
| Parameter values can have significant, but unintuitive effects on model fit   | Allowing models to accept vectors of possible values may improve fit and provide a better understanding of the interacting relationships between model parameters.   |
| **Process** ||
| Uncertainty in depth is not represented | Estimates of overlap within composite cores, based on correlation from geochemical or visual cues may be included |
| Models don't use secondary information. | Secondary parameters may be used to indicate rapid changes in memory, or accumulation. |
| **Product** ||
| Age models require significant oversight |  Codify certain decisions, provide testing suites for fit parameters. |
| Ensuring that model outputs are clear and the decisions made about parameters are justified.  | Improved uptake of reproducible workflows and use of provenance tools. |
| Managing records with multiple age models, with varying parameters. | Decision making processes must be codified for the use or rejection of models when multiple "best-fit" models exist with varying parameters. |

## Future development

The tools and techniques for generating age models continues to increase.  BChron [@parnell2016package], Bacon STAN ([https://github.com/andrewdolman/baconr/]()), CSciBox ([https://www.cs.colorado.edu/~lizb/cscience.html]()), OxCal [@ramsey2013recent] all exist as alternative tools, and geochronR ([https://github.com/nickmckay/GeoChronR]()) exists as a resource for managing age models in the R programming environment.  It is unlikely that the community will select a single resource for modeling chronologies, but the adoption of certain standards across the paleogeosciences can help improve interoperability of data, and ultimately, lead to improvements in our overall understanding of many of the exciting research questions that drive us as a community and as individuals.

# Conclusions

Large-scale synthesis using Quaternary paleoecological records may require the development of purpose-built chronologies.  Careful attention to chronology development can result in substantial changes to site-level chronologies, and systematic offsets at a regional scale.  The development of new chronologies must consider choices at the scale of individual chronological controls, at the scale of the site, and at regional scales.  Using a matrix of parameters with empirical support, and choosing among multiple realizations is likely the most effective method for developing high quality chronologies for use in synthesis research.  Once models have been constructed, a secondary goal is the preservation and encoding of the choices that went into model development.  High quality paleoecological research increasingly depends on high quality chronological support, and as such, researchers must pass on the information they have gained, in developing site level and regional chronologies, in a well supported and effectively communicated manner.

# Acknowledgements

This work is a contribution of the PalEON Project (http://paleon-project.org).  SJG would like to thank support from NSG, CEG and AMG. SJG is supported through NSF Grants NSF-1541002, NSF-1550855, NSF-1241868, and NSF-1550707.  This paper was improved through discussions with many in the paleoecological community, and would not have been possible without the significant contributions of all those who have contributed to the Neotoma Paleoecological Database.

# References
